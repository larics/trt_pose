{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419bf9e8-9b32-4316-93ac-2adc18cbd9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_pose\n",
    "import trt_pose.coco\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0c5dec-b18a-4cd7-855b-52a4c936a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import topology\n",
    "with open('/home/trt_pose/tasks/human_pose/human_pose.json', 'r') as f: \n",
    "    human_pose = json.load(f)\n",
    "\n",
    "# Import topology \n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b321b6-4c2e-4664-9fdf-a15002974ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50df7179-0a8f-4466-9c9d-64c0e98b2b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "MODEL_WEIGHTS = '/home/trt_pose/tasks/human_pose/resnet18_baseline_att_224x224_A_epoch_249.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2081ea-4ad7-49f9-bf2e-ffb02e546df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "\n",
    "data = torch.zeros((1, 3, HEIGHT, WIDTH)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbfa0b6-1fdb-4a43-9cae-958e2f972cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch2trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5054c8-0722-4ef3-b2ee-ae9a3c200cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trt = torch2trt.torch2trt(model, [data], fp16_mode=True, max_workspace_size=1<<25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a220ac6d-8c86-48fd-805b-bf5e3dbb6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "torch.save(model_trt.state_dict(), OPTIMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbac4aa2-3b50-4c80-8d8e-365241cbba55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch2trt import TRTModule\n",
    "\n",
    "OPTIMIZED_MODEL = '/home/trt_pose/tasks/human_pose/resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85da1cd4-5433-46f1-afb9-3cecf9d49605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.47543058541248\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "t0 = time.time()\n",
    "torch.cuda.current_stream().synchronize()\n",
    "for i in range(50): \n",
    "    y = model_trt(data)\n",
    "torch.cuda.current_stream().synchronize()\n",
    "t1 = time.time()\n",
    "print(50.0/(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e3de28-9c77-4ceb-bd36-4eab3ffaad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa6346a-1d19-42f7-a452-fd508a67c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba7693-432b-4ba2-b04a-ee73d940903a",
   "metadata": {},
   "source": [
    "## Necessary to use jetcam package \n",
    "\n",
    "Find [jetcam](https://github.com/NVIDIA-AI-IOT/jetcam). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39ad4f6-e0ca-41a1-bbcd-9b196fd07758",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jetcam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjetcam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01musb_camera\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m USBCamera\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from jetcam.csi_camera import CSICamera\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjetcam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bgr8_to_jpeg\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jetcam'"
     ]
    }
   ],
   "source": [
    "from jetcam.usb_camera import USBCamera\n",
    "# from jetcam.csi_camera import CSICamera\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "camera = USBCamera(width=WIDTH, height=HEIGHT, capture_fps=30)\n",
    "# camera = CSICamera(width=WIDTH, height=HEIGHT, capture_fps=30)\n",
    "\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272936b-b5c1-4f52-bd51-98f9774a6137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
